{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello welcome, to Mohsin's NLP tutorials.\n",
      "Please do watch the entire course!\" to become expert in NLP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\" Hello welcome, to Mohsin's NLP tutorials.\n",
    "Please do watch the entire course!\" to become expert in NLP\n",
    "\"\"\"\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## paraghraphs --> Sentence \n",
    "from nltk.tokenize import sent_tokenize\n",
    "document = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello welcome, to Mohsin's NLP tutorials.\n",
      "Please do watch the entire course!\"\n",
      "to become expert in NLP\n"
     ]
    }
   ],
   "source": [
    "for sentence in document:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Mohsin',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " \"''\",\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "## Paragraph --> words\n",
    "### Sentence --> wrods\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', ',', 'to', 'Mohsin', \"'s\", 'NLP', 'tutorials', '.']\n",
      "['Please', 'do', 'watch', 'the', 'entire', 'course', '!', \"''\"]\n",
      "['to', 'become', 'expert', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "for sentence in document:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Mohsin',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!\"',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree back work tokenization\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Mohsin',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'tutorials.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " \"''\",\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Problem\n",
    "# Comment of product is positive review or negative review\n",
    "# Reviews --> [eating , eats , eaten]--> eat , [going , goes , gone]--> go\n",
    "\n",
    "words = [\"eating\" , \"eats\" , \"eaten\" , \"going\" , \"goes\" , \"gone\" , \"programming\" , \"programms\" , \"history\" , \"finally\" , \"finalized\" ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eats--->eat\n",
      "eaten--->eaten\n",
      "going--->go\n",
      "goes--->goe\n",
      "gone--->gone\n",
      "programming--->program\n",
      "programms--->programm\n",
      "history--->histori\n",
      "finally--->final\n",
      "finalized--->final\n"
     ]
    }
   ],
   "source": [
    "# Porterstemmer\n",
    "\n",
    "from nltk. stem import PorterStemmer\n",
    "\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(word+ \"--->\" + stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"congratulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"sitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RegexStemmer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$|ed$', min=5)\n",
    "reg_stemmer.stem(\"eated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowboll Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---> eat\n",
      "eats---> eat\n",
      "eaten---> eaten\n",
      "going---> go\n",
      "goes---> goe\n",
      "gone---> gone\n",
      "programming---> program\n",
      "programms---> programm\n",
      "history---> histori\n",
      "finally---> final\n",
      "finalized---> final\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow_ball = SnowballStemmer('english')\n",
    "\n",
    "for word in words:\n",
    "    print(word + \"---> \"+ snow_ball.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_ball.stem(\"fairly\"),snow_ball.stem(\"Sportingly\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "- Lemmatization technique like stemmer. \n",
    "- Output after lemmatization is called Lemma \n",
    "- Which is the root word than root stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q&A , chatbots , text sumariozation\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "'''\n",
    "POS- Noun-n\n",
    "verb - v\n",
    "adjective -a\n",
    "adverb - r\n",
    "'''\n",
    "\n",
    "lemmatizer.lemmatize(\"going\" , pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eating\n",
      "eats--->eats\n",
      "eaten--->eaten\n",
      "going--->going\n",
      "goes--->go\n",
      "gone--->gone\n",
      "programing--->programing\n",
      "programs--->program\n",
      "history--->history\n",
      "finally--->finally\n",
      "finalized--->finalized\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\" , \"eats\" , \"eaten\" , \"going\" , \"goes\" , \"gone\" , \"programing\" , \"programs\" , \"history\" , \"finally\" , \"finalized\" ]\n",
    "\n",
    "for word in words:\n",
    "    print(word+ \"--->\"+lemmatizer.lemmatize(word , pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\" Ramadan is the ninth month of the Islamic lunar calendar and is observed by Muslims worldwide as a month of fasting, prayer, reflection, and community. It is a time for spiritual growth, self-discipline, and empathy for those less fortunate. During Ramadan, Muslims abstain from food, drink, and other physical needs from dawn until sunset, focusing instead on spiritual pursuits such as reciting the Quran, performing extra prayers, and engaging in acts of charity. The fast is broken each evening with a meal called iftar, often shared with family and friends, and the month culminates in the joyous celebration of Eid al-Fitr, a time of feasting, prayer, and giving thanks for the blessings of Ramadan. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['och', 'det', 'att', 'i', 'en', 'jag', 'hon', 'som', 'han', 'på', 'den', 'med', 'var', 'sig', 'för', 'så', 'till', 'är', 'men', 'ett', 'om', 'hade', 'de', 'av', 'icke', 'mig', 'du', 'henne', 'då', 'sin', 'nu', 'har', 'inte', 'hans', 'honom', 'skulle', 'hennes', 'där', 'min', 'man', 'ej', 'vid', 'kunde', 'något', 'från', 'ut', 'när', 'efter', 'upp', 'vi', 'dem', 'vara', 'vad', 'över', 'än', 'dig', 'kan', 'sina', 'här', 'ha', 'mot', 'alla', 'under', 'någon', 'eller', 'allt', 'mycket', 'sedan', 'ju', 'denna', 'själv', 'detta', 'åt', 'utan', 'varit', 'hur', 'ingen', 'mitt', 'ni', 'bli', 'blev', 'oss', 'din', 'dessa', 'några', 'deras', 'blir', 'mina', 'samma', 'vilken', 'er', 'sådan', 'vår', 'blivit', 'dess', 'inom', 'mellan', 'sådant', 'varför', 'varje', 'vilka', 'ditt', 'vem', 'vilket', 'sitta', 'sådana', 'vart', 'dina', 'vars', 'vårt', 'våra', 'ert', 'era', 'vilkas']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('swedish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer  = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph --- > Sentences\n",
    "sentence = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ramadan ninth month islam lunar calendar observ muslim worldwid month fast , prayer , reflect , commun .',\n",
       " 'it time spiritu growth , self-disciplin , empathi less fortun .',\n",
       " 'dure ramadan , muslim abstain food , drink , physic need dawn sunset , focus instead spiritu pursuit recit quran , perform extra prayer , engag act chariti .',\n",
       " 'the fast broken even meal call iftar , often share famili friend , month culmin joyou celebr eid al-fitr , time feast , prayer , give thank bless ramadan .']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply stopwords And filter and then apply stemming\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words= nltk.word_tokenize(sentence[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i] = ' '.join(words) # converting  all the list of  words into sentences\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnowballStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ramadan ninth month islam lunar calendar observ muslim worldwid month fast , prayer , reflect , commun .',\n",
       " 'time spiritu growth , self-disciplin , empathi less fortun .',\n",
       " 'dure ramadan , muslim abstain food , drink , physic need dawn sunset , focus instead spiritu pursuit recit quran , perform extra prayer , engag act chariti .',\n",
       " 'fast broken even meal call iftar , often share famili friend , month culmin joyou celebr eid al-fitr , time feast , prayer , give thank bless ramadan .']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply stopwords And filter and then apply Snowball stemming\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words= nltk.word_tokenize(sentence[i])\n",
    "    words = [snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i] = ' '.join(words) # converting  all the list of  words into sentences\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization ( stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ramadan ninth month islam lunar calendar observ muslim worldwid month fast , prayer , reflect , commun .',\n",
       " 'time spiritu growth , self-disciplin , empathi less fortun .',\n",
       " 'dure ramadan , muslim abstain food , drink , physic need dawn sunset , focus instead spiritu pursuit recit quran , perform extra prayer , engag act chariti .',\n",
       " 'fast break even meal call iftar , often share famili friend , month culmin joyou celebr eid al-fitr , time feast , prayer , give thank bless ramadan .']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply stopwords And filter and then apply  Lemmetization stemming\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words= nltk.word_tokenize(sentence[i])\n",
    "    words = [lemmatizer.lemmatize(word , pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i] = ' '.join(words) # converting  all the list of  words into sentences\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Ramadan is the ninth month of the Islamic lunar calendar and is observed by Muslims worldwide as a month of fasting, prayer, reflection, and community.',\n",
       " 'It is a time for spiritual growth, self-discipline, and empathy for those less fortunate.',\n",
       " 'During Ramadan, Muslims abstain from food, drink, and other physical needs from dawn until sunset, focusing instead on spiritual pursuits such as reciting the Quran, performing extra prayers, and engaging in acts of charity.',\n",
       " 'The fast is broken each evening with a meal called iftar, often shared with family and friends, and the month culminates in the joyous celebration of Eid al-Fitr, a time of feasting, prayer, and giving thanks for the blessings of Ramadan.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = nltk.sent_tokenize(paragraph)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ramadan', 'NNP'), ('ninth', 'JJ'), ('month', 'NN'), ('Islamic', 'NNP'), ('lunar', 'NN'), ('calendar', 'NN'), ('observed', 'VBD'), ('Muslims', 'NNP'), ('worldwide', 'IN'), ('month', 'NN'), ('fasting', 'NN'), (',', ','), ('prayer', 'NN'), (',', ','), ('reflection', 'NN'), (',', ','), ('community', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('time', 'NN'), ('spiritual', 'JJ'), ('growth', 'NN'), (',', ','), ('self-discipline', 'JJ'), (',', ','), ('empathy', 'JJ'), ('less', 'JJR'), ('fortunate', 'NN'), ('.', '.')]\n",
      "[('During', 'IN'), ('Ramadan', 'NNP'), (',', ','), ('Muslims', 'NNP'), ('abstain', 'VBP'), ('food', 'NN'), (',', ','), ('drink', 'NN'), (',', ','), ('physical', 'JJ'), ('needs', 'NNS'), ('dawn', 'NN'), ('sunset', 'NN'), (',', ','), ('focusing', 'VBG'), ('instead', 'RB'), ('spiritual', 'JJ'), ('pursuits', 'NNS'), ('reciting', 'VBG'), ('Quran', 'NNP'), (',', ','), ('performing', 'VBG'), ('extra', 'JJ'), ('prayers', 'NNS'), (',', ','), ('engaging', 'VBG'), ('acts', 'NNS'), ('charity', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('fast', 'JJ'), ('broken', 'JJ'), ('evening', 'NN'), ('meal', 'NN'), ('called', 'VBN'), ('iftar', 'NN'), (',', ','), ('often', 'RB'), ('shared', 'VBN'), ('family', 'NN'), ('friends', 'NNS'), (',', ','), ('month', 'NN'), ('culminates', 'NNS'), ('joyous', 'JJ'), ('celebration', 'NN'), ('Eid', 'NNP'), ('al-Fitr', 'NN'), (',', ','), ('time', 'NN'), ('feasting', 'NN'), (',', ','), ('prayer', 'NN'), (',', ','), ('giving', 'VBG'), ('thanks', 'NNS'), ('blessings', 'NNS'), ('Ramadan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# we will find out the pos tag\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    words= nltk.word_tokenize(sentence[i])\n",
    "    words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "    #sentence[i] = ' '.join(words) # converting  all the list of  words into sentences\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"Taj mahal is beautiful mounment\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Taj', 'NNP'),\n",
       " ('mahal', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('mounment', 'NN')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform POS tagging on the list of words\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPerson eg : Muhsin Ahmad\\nPlace or Location : London\\nDateeg: September , 23-09-2023\\nTime eg: 04:30pm\\nMoney eg: 1 million dollar\\nOrginazation eg : Ineuron Privated Limited\\nPercent eg : 20% , twenty percent\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentece = \"The Eiffel Tower was built from 1887 to 1889 by French enginneer Gustva Eiffel , whose company specialized in building metal framworks and structures.  \"\n",
    "\n",
    "\"\"\"\n",
    "Person eg : Muhsin Ahmad\n",
    "Place or Location : London\n",
    "Dateeg: September , 23-09-2023\n",
    "Time eg: 04:30pm\n",
    "Money eg: 1 million dollar\n",
    "Orginazation eg : Ineuron Privated Limited\n",
    "Percent eg : 20% , twenty percent\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words = nltk.word_tokenize(sentece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_element = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nltk.ne_chunk(tag_element).draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
